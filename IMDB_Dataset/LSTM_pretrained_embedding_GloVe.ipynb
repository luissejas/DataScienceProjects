{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_pretrained_embedding_GloVe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBx8Ldhhub1"
      },
      "source": [
        "!pip install tensorflow-datasets > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MCTLR9giPrS",
        "outputId": "d5119412-1578-4240-bab0-cb7217f31dd5"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stemmer = WordNetLemmatizer()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHcSl6-cit5Z"
      },
      "source": [
        "(ds_train,ds_test),ds_info = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train\",\"test\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnjVxyLqlF9p"
      },
      "source": [
        "df_train = tfds.as_dataframe(ds_train, ds_info)\n",
        "df_test = tfds.as_dataframe(ds_test, ds_info)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU8aJ7sbi8bh"
      },
      "source": [
        "def clean_entry(text_list):\n",
        "  str_list = []\n",
        "  for text in text_list:\n",
        "    str_text = str(text)\n",
        "    str_text_lim = len(str_text)-1\n",
        "    str_text = str_text[1:str_text_lim]\n",
        "    str_list.append(str_text)\n",
        "  return html_term_remover(str_list)\n",
        "\n",
        "def prepare_for_ai(df_col):\n",
        "  list_to_return = df_col.tolist()\n",
        "  return clean_entry(list_to_return)\n",
        "\n",
        "def html_term_remover(df_list: list):\n",
        "  return_list = []\n",
        "  for i in df_list:\n",
        "    b_soup = BeautifulSoup(i, 'html.parser')\n",
        "    return_list.append(b_soup.get_text())\n",
        "  return apply_re(return_list)\n",
        "\n",
        "def apply_re(str_list):\n",
        "  re_list = []\n",
        "  for text in str_list:\n",
        "    text = re.sub(\"[^0-9A-Za-z ]\", \"\", text)\n",
        "    re_list.append(text)\n",
        "  return remove_integer(re_list)\n",
        "\n",
        "def remove_integer(str_list):\n",
        "  re_list = []\n",
        "  int_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "  for text in str_list:\n",
        "    sentence_list = []\n",
        "    new_text = text.split()\n",
        "    for word in new_text:\n",
        "      if word not in int_list:\n",
        "        sentence_list.append(word)\n",
        "    re_list.append(' '.join(sentence_list))\n",
        "  return return_lower_text(re_list)\n",
        "\n",
        "def return_lower_text(str_list):\n",
        "  re_list = []\n",
        "  for word in str_list:\n",
        "    re_list.append(word.lower())\n",
        "  return apply_nltk(re_list)\n",
        "\n",
        "def apply_nltk(text):\n",
        "  return_list = []\n",
        "  for elem in text:\n",
        "    tokens = word_tokenize(elem)\n",
        "    working_list = []\n",
        "    for word in tokens:\n",
        "      if len(word) > 3 and word not in stopwords:\n",
        "        working_list.append(stemmer.lemmatize(word))\n",
        "    return_list.append(' '.join(working_list))\n",
        "  return return_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdp_2f9ajc7Z"
      },
      "source": [
        "x_train = prepare_for_ai(df_train['text'])\n",
        "x_test = prepare_for_ai(df_test['text'])\n",
        "y_train = df_train['label'].values.tolist()\n",
        "y_test = df_test['label'].values.tolist()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDc-ANxolimA"
      },
      "source": [
        "x_all = x_train.copy()\n",
        "x_all.extend(x_test)\n",
        "\n",
        "y_all = y_train.copy()\n",
        "y_all.extend(y_test)\n",
        "\n",
        "assert len(y_all) == len(x_all)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFdBCWcIlu0Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlNBc6x_m_LN"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=750, output_sequence_length=200, standardize=None)\n",
        "vectorizer.adapt(x_all)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLOQ_HOxpGCM"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfMpJOdJpF5e"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ccCJoI2pf39",
        "outputId": "82afc61b-bc12-4599-d20d-d5d807f75a86"
      },
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "with open(\"/content/glove.6B.200d.txt\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDrPf9bXpf1k",
        "outputId": "924ae1ba-2548-4405-e754-721b1d709cb4"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 200\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 746 words (4 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfcM9oYwvPT6"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding,LSTM, SpatialDropout1D, Input, Dropout\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgZkU9aa4j5c",
        "outputId": "cd2852dc-7b64-4f05-f467-bb4799d0f0e0"
      },
      "source": [
        "inputs = Input(shape=(200,))\n",
        "embedding_layer = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix))(inputs)\n",
        "dropout_1 = SpatialDropout1D(0.50)(embedding_layer)\n",
        "lstm_layer = LSTM(75)(dropout_1)\n",
        "dropout_2 = Dropout(0.05)(lstm_layer)\n",
        "d_layer = Dense(10, activation=\"relu\")(dropout_2)\n",
        "out_layer = Dense(1, activation='sigmoid')(d_layer)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=out_layer)\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 200)          150400    \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 200, 200)         0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 75)                82800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 75)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                760       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 233,971\n",
            "Trainable params: 233,971\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-gcQbnT1GNZ"
      },
      "source": [
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he4clxnK1nqi"
      },
      "source": [
        "loss = BinaryCrossentropy(from_logits=False)\n",
        "metric = BinaryAccuracy(name='accuracy')\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metric)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YChl_xLgpFx8"
      },
      "source": [
        "x_all_vectorized = vectorizer(np.array([[s] for s in x_all])).numpy()\n",
        "y_all_vectorized = np.asarray(y_all)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4fVdtwksxT-"
      },
      "source": [
        "x_train_model, x_test_model, y_train_model, y_test_model = train_test_split(x_all_vectorized, y_all_vectorized, test_size=0.2, random_state=35)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oN3Gajg2g97",
        "outputId": "12b72ac6-4cb6-4aba-fe33-e3c782078aa2"
      },
      "source": [
        "model.fit(x_train_model, y_train_model, epochs=15, batch_size=32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1250/1250 [==============================] - 17s 12ms/step - loss: 0.6857 - accuracy: 0.5388\n",
            "Epoch 2/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6879 - accuracy: 0.5223\n",
            "Epoch 3/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.4895 - accuracy: 0.7538\n",
            "Epoch 4/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3709 - accuracy: 0.8368\n",
            "Epoch 5/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3505 - accuracy: 0.8473\n",
            "Epoch 6/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3405 - accuracy: 0.8522\n",
            "Epoch 7/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3308 - accuracy: 0.8571\n",
            "Epoch 8/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3249 - accuracy: 0.8589\n",
            "Epoch 9/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3152 - accuracy: 0.8651\n",
            "Epoch 10/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3104 - accuracy: 0.8680\n",
            "Epoch 11/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3045 - accuracy: 0.8714\n",
            "Epoch 12/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2981 - accuracy: 0.8723\n",
            "Epoch 13/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2907 - accuracy: 0.8753\n",
            "Epoch 14/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2854 - accuracy: 0.8788\n",
            "Epoch 15/15\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2799 - accuracy: 0.8831\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe48e7c3a50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kd21msK7_u7",
        "outputId": "f48b2f38-a6c4-41ce-cae0-0f4246f57913"
      },
      "source": [
        "model.evaluate(x_test_model, y_test_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3690 - accuracy: 0.8475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3689700663089752, 0.8475000262260437]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}