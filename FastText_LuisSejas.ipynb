{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastText_LuisSejas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jqgreu9ut2S"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Author: Luis Sejas \n",
        "\n",
        "Student ID: 8440116"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRB-mbA5eUG"
      },
      "source": [
        "# Before the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbrD3-QfvtOA"
      },
      "source": [
        "## Part 1: Loading and Seeing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EpS9gKEbZjh"
      },
      "source": [
        "%%capture\n",
        "!pip install tensorflow-datasets > /dev/null\n",
        "!pip install fasttext"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnWuY26nbk2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5323472-3a1f-4094-a4ce-0e10f97b4ae6"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import fasttext\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stemmer = WordNetLemmatizer()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW87hHg1bleh"
      },
      "source": [
        "(ds_train,ds_test),ds_info = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train\",\"test\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QnSCe7Ec0v6"
      },
      "source": [
        "df_train = tfds.as_dataframe(ds_train, ds_info)\n",
        "df_test = tfds.as_dataframe(ds_test, ds_info)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j7Mr7GDvz_c"
      },
      "source": [
        "## Part 2: Pre-processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb1aWT5Xv_5a"
      },
      "source": [
        "I have noticed that the reviews start with b' or with b\" and ' or \" at the end, among other stuff.\n",
        "\n",
        "The aim here is to clean the data to train an algorithm that will automatically detect the sentiment correctly Ideally, even ambiguous text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNKn_XjtxNth"
      },
      "source": [
        "Below is a series of formulas to clean the reviews.\n",
        "\n",
        "Keep in mind this is only the beginning, therefore some deep cleaning will not be employed at this stage and yes on the other ones.\n",
        "\n",
        "This pre-processing will be preserved for comparison purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsZ0Ocz3hAU2"
      },
      "source": [
        "def clean_entry(text_list):\n",
        "  str_list = []\n",
        "  for text in text_list:\n",
        "    str_text = str(text)\n",
        "    str_text_lim = len(str_text)-1\n",
        "    str_text = str_text[1:str_text_lim]\n",
        "    str_list.append(str_text)\n",
        "  return html_term_remover(str_list)\n",
        "\n",
        "def prepare_for_ai(df_col):\n",
        "  list_to_return = df_col.tolist()\n",
        "  return clean_entry(list_to_return)\n",
        "\n",
        "def html_term_remover(df_list: list):\n",
        "  return_list = []\n",
        "  for i in df_list:\n",
        "    b_soup = BeautifulSoup(i, 'html.parser')\n",
        "    return_list.append(b_soup.get_text())\n",
        "  return apply_re(return_list)\n",
        "\n",
        "def apply_re(str_list):\n",
        "  re_list = []\n",
        "  for text in str_list:\n",
        "    text = re.sub(\"[^0-9A-Za-z ]\", \"\", text)\n",
        "    re_list.append(text)\n",
        "  return remove_integer(re_list)\n",
        "\n",
        "def remove_integer(str_list):\n",
        "  re_list = []\n",
        "  int_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "  for text in str_list:\n",
        "    sentence_list = []\n",
        "    new_text = text.split()\n",
        "    for word in new_text:\n",
        "      if word not in int_list:\n",
        "        sentence_list.append(word)\n",
        "    re_list.append(' '.join(sentence_list))\n",
        "  return return_lower_text(re_list)\n",
        "\n",
        "def return_lower_text(str_list):\n",
        "  re_list = []\n",
        "  for word in str_list:\n",
        "    re_list.append(word.lower())\n",
        "  return re_list\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMYFY-145p1h"
      },
      "source": [
        "# The following variables are base and every model will have its own adaptations\n",
        "\n",
        "x_train = prepare_for_ai(df_train['text'])\n",
        "x_test = prepare_for_ai(df_test['text'])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5loPSkdkhtt"
      },
      "source": [
        "y_train_fasttext = y_train.values.tolist()\n",
        "y_test_fasttext = y_test.values.tolist()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwP8hR0lK7V"
      },
      "source": [
        "y_train_converted = []\n",
        "y_test_converted = []\n",
        "\n",
        "for label in y_train_fasttext:\n",
        "  if label == 1:\n",
        "    fasttext_label = \"__label__positive\"\n",
        "    y_train_converted.append(fasttext_label)\n",
        "  if label == 0:\n",
        "    fasttext_label = \"__label__negative\"\n",
        "    y_train_converted.append(fasttext_label)\n",
        "\n",
        "for label in y_test_fasttext:\n",
        "  if label == 1:\n",
        "    fasttext_label = \"__label__positive\"\n",
        "    y_test_converted.append(fasttext_label)\n",
        "  if label == 0:\n",
        "    fasttext_label = \"__label__negative\"\n",
        "    y_test_converted.append(fasttext_label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jejzjymnpoQ"
      },
      "source": [
        "def apply_nltk(text):\n",
        "  return_list = []\n",
        "  for elem in text:\n",
        "    tokens = word_tokenize(elem)\n",
        "    working_list = []\n",
        "    for word in tokens:\n",
        "      if len(word) > 3 and word not in stopwords:\n",
        "        working_list.append(stemmer.lemmatize(word))\n",
        "    return_list.append(' '.join(working_list))\n",
        "  return return_list\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRqdaBNipy2m"
      },
      "source": [
        "x_train_fasttext = apply_nltk(x_train)\n",
        "x_test_fasttext = apply_nltk(x_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZG1LRh23tbn"
      },
      "source": [
        "x_train_joined = []\n",
        "for index in range(0, len(x_train_fasttext)):\n",
        "  new_text = y_train_converted[index] + \" \" + x_train_fasttext[index]\n",
        "  x_train_joined.append(new_text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNwXxXxR4dq-"
      },
      "source": [
        "x_test_joined = []\n",
        "for index in range(0, len(x_test_fasttext)):\n",
        "  new_text = y_test_converted[index] + \" \" + x_test_fasttext[index]\n",
        "  x_test_joined.append(new_text)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HaDqFjcAvsK"
      },
      "source": [
        "np.savetxt(\"x_test_ft.txt\", x_test_joined, delimiter=\"\\n\", fmt=\"%s\")\n",
        "np.savetxt(\"x_train_ft.txt\", x_train_joined, delimiter=\"\\n\", fmt=\"%s\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u83ForyNCQ73"
      },
      "source": [
        "model = fasttext.train_supervised(input=\"x_train_ft.txt\", lr=0.1, epoch=5)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-fnZVCxDGzf",
        "outputId": "6fa880fa-5b63-4411-bbad-6a00301488a9"
      },
      "source": [
        "model.test(\"x_test_ft.txt\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 0.87096, 0.87096)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}