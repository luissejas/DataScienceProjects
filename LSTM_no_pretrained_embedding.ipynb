{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_no_pretrained_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBx8Ldhhub1"
      },
      "source": [
        "!pip install tensorflow-datasets > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MCTLR9giPrS",
        "outputId": "ff25c25a-947d-4c80-e90d-a895e2da84c8"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stemmer = WordNetLemmatizer()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHcSl6-cit5Z"
      },
      "source": [
        "(ds_train,ds_test),ds_info = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train\",\"test\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnjVxyLqlF9p"
      },
      "source": [
        "df_train = tfds.as_dataframe(ds_train, ds_info)\n",
        "df_test = tfds.as_dataframe(ds_test, ds_info)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU8aJ7sbi8bh"
      },
      "source": [
        "def clean_entry(text_list):\n",
        "  str_list = []\n",
        "  for text in text_list:\n",
        "    str_text = str(text)\n",
        "    str_text_lim = len(str_text)-1\n",
        "    str_text = str_text[1:str_text_lim]\n",
        "    str_list.append(str_text)\n",
        "  return html_term_remover(str_list)\n",
        "\n",
        "def prepare_for_ai(df_col):\n",
        "  list_to_return = df_col.tolist()\n",
        "  return clean_entry(list_to_return)\n",
        "\n",
        "def html_term_remover(df_list: list):\n",
        "  return_list = []\n",
        "  for i in df_list:\n",
        "    b_soup = BeautifulSoup(i, 'html.parser')\n",
        "    return_list.append(b_soup.get_text())\n",
        "  return apply_re(return_list)\n",
        "\n",
        "def apply_re(str_list):\n",
        "  re_list = []\n",
        "  for text in str_list:\n",
        "    text = re.sub(\"[^0-9A-Za-z ]\", \"\", text)\n",
        "    re_list.append(text)\n",
        "  return remove_integer(re_list)\n",
        "\n",
        "def remove_integer(str_list):\n",
        "  re_list = []\n",
        "  int_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "  for text in str_list:\n",
        "    sentence_list = []\n",
        "    new_text = text.split()\n",
        "    for word in new_text:\n",
        "      if word not in int_list:\n",
        "        sentence_list.append(word)\n",
        "    re_list.append(' '.join(sentence_list))\n",
        "  return return_lower_text(re_list)\n",
        "\n",
        "def return_lower_text(str_list):\n",
        "  re_list = []\n",
        "  for word in str_list:\n",
        "    re_list.append(word.lower())\n",
        "  return apply_nltk(re_list)\n",
        "\n",
        "def apply_nltk(text):\n",
        "  return_list = []\n",
        "  for elem in text:\n",
        "    tokens = word_tokenize(elem)\n",
        "    working_list = []\n",
        "    for word in tokens:\n",
        "      if len(word) > 3 and word not in stopwords:\n",
        "        working_list.append(stemmer.lemmatize(word))\n",
        "    return_list.append(' '.join(working_list))\n",
        "  return return_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdp_2f9ajc7Z"
      },
      "source": [
        "x_train = prepare_for_ai(df_train['text'])\n",
        "x_test = prepare_for_ai(df_test['text'])\n",
        "y_train = df_train['label'].values.tolist()\n",
        "y_test = df_test['label'].values.tolist()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDc-ANxolimA"
      },
      "source": [
        "x_all = x_train.copy()\n",
        "x_all.extend(x_test)\n",
        "\n",
        "y_all = y_train.copy()\n",
        "y_all.extend(y_test)\n",
        "\n",
        "assert len(y_all) == len(x_all)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFdBCWcIlu0Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRaTK4IfmcR5"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=750, split=' ')\n",
        "tokenizer.fit_on_texts(x_all)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LXiCuknsBTc"
      },
      "source": [
        "x_as_sequence = tokenizer.texts_to_sequences(x_all)\n",
        "x_as_sequence = pad_sequences(x_as_sequence)\n",
        "\n",
        "y_all_array = np.asarray(y_all)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoFJI2cAsvTC",
        "outputId": "25a3bf08-2c05-4536-fb6f-7a09e08827c3"
      },
      "source": [
        "print(x_as_sequence.shape)\n",
        "print(y_all_array.shape)\n",
        "print(type(x_as_sequence))\n",
        "print(type(y_all_array))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 514)\n",
            "(50000,)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4fVdtwksxT-"
      },
      "source": [
        "x_train_padded, x_test_padded, y_train_new, y_test_new = train_test_split(x_as_sequence, y_all_array, test_size=0.2, random_state=35)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfcM9oYwvPT6"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding,LSTM, SpatialDropout1D, Input, Dropout"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_092qGeHvqTB",
        "outputId": "59d24845-5da5-49c8-e593-e9aaef3e028b"
      },
      "source": [
        "inputs = Input(shape=(514,))\n",
        "embedding_layer = Embedding(751, 150, input_length=553)(inputs)\n",
        "dropout_1 = SpatialDropout1D(0.35)(embedding_layer)\n",
        "lstm_layer = LSTM(75)(dropout_1)\n",
        "dropout_2 = Dropout(0.15)(lstm_layer)\n",
        "d_layer = Dense(10, activation=\"relu\")(dropout_2)\n",
        "out_layer = Dense(1, activation='sigmoid')(d_layer)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=out_layer, name=\"lstm_classifier\")\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstm_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 514)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 514, 150)          112650    \n",
            "                                                                 \n",
            " spatial_dropout1d_1 (Spatia  (None, 514, 150)         0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 75)                67800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 75)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                760       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 181,221\n",
            "Trainable params: 181,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-gcQbnT1GNZ"
      },
      "source": [
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he4clxnK1nqi"
      },
      "source": [
        "loss = BinaryCrossentropy(from_logits=False)\n",
        "metric = BinaryAccuracy(name='accuracy')\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metric)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oN3Gajg2g97",
        "outputId": "6c3a50d1-b0b2-4300-c7d2-f5fe7ce6464e"
      },
      "source": [
        "model.fit(x_train_padded, y_train_new, epochs=10, batch_size=32)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 41s 31ms/step - loss: 0.4080 - accuracy: 0.8133\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3521 - accuracy: 0.8484\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.3364 - accuracy: 0.8530\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3235 - accuracy: 0.8601\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 0.3148 - accuracy: 0.8645\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3054 - accuracy: 0.8686\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.2953 - accuracy: 0.8717\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.2849 - accuracy: 0.8787\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.2757 - accuracy: 0.8815\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 41s 32ms/step - loss: 0.2637 - accuracy: 0.8873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a8d445cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kd21msK7_u7",
        "outputId": "2b9202d9-a178-4abd-df64-b63c6737ee9e"
      },
      "source": [
        "model.evaluate(x_test_padded, y_test_new)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 5s 14ms/step - loss: 0.3969 - accuracy: 0.8438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.39689555764198303, 0.8438000082969666]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}