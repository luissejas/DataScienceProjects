{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-747367f9-7b2a-473e-b09c-f18008833ce5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Text Generator Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-afc8ba9c-51fc-4079-9bbb-a1a8a9ee85f5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Note:\n",
    "- This notebook serves as the main notebook of the project in terms of its details of related concepts and model training, for the other two notebooks for training models have identical structures of processes.\n",
    "- For details of the models for the other two age groups, please refer to their corresponding training notebooks\n",
    "- For preprocessing, please refer to **(1) Initial analysis of dataset.ipynb** and the three notebooks with the prefix `Preprocessing` inside the folder named `Initial processing`.\n",
    "- For further conclusion of all three models, please refer to **Model comparison and conclusion.ipynb**\n",
    "- The numbering of the original jupyter notebook file is not ordered consecutively due to various changes made during the modelling. For reading convenience in its .pdf file, the number orders of cells have been changed using Latex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-cfd5fee3-b6cc-4f15-8e8d-6626461dd24d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Introduction\n",
    "Considering the great size of the original dataset and time as well as computing power constraints, only a sample of it was used to train the model. Samples of two different sizes were generated for each age group's dataset: one of the length 10,000 (sentences) and one of the length 5,000 (sentences). \n",
    "\n",
    "At first, our group focus on finding a way to train the larger sample (with 10,000 sentences). After several experiments, we discover that there are two main ways of doing so: \n",
    "\n",
    "(1) using CPU to train all 10,000 observations all at once, which takes over 5 minutes for each epoch. \n",
    "Considering that the models need a lot of adjustments and we don't have much time, this approach was not considered.\n",
    "\n",
    "(2) using GPU to train the 10,000 observations in 'batches' (i.e. first train the first 3,000 instances, then train the next 3,000 instances, and so on), for, the GPU, although can speed up the training a lot, has limitations on the training data size due to a memory allocation issue.\n",
    "As shown in **(3.1.1) Multi Batch GPU-Under_20.ipynb** inside the folder `Multi Batch Trial`, this approach did not produce satisfying results, which is possibly caused by the small data size and the nature of the original dataset (for more details please refer to the said file).\n",
    "\n",
    "Therefore, we later turn to train the second sample which is half the size of the first one, as it is the limit of our GPU without encountering any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00005-16b437d8-d810-4de6-ae10-8c5640693ded",
    "deepnote_cell_type": "code",
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-2676fbd2-be16-4418-bbb5-d78d5672fcef",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## I. Tokenization\n",
    "\n",
    "To prepare the text for the model training, its words need to be encoded first using a process called **tokenization**. Tokenization creates a vocabulary index based on word frequency. \n",
    "It will create a dictionary in which every word of the input text gets a unique integer value (0 is reserved for padding). Lower integer means more frequent word (often the first few are stop words because they appear a lot). Later, each piece of text is transformed to a sequence of integers, according to the dictionary created above. \n",
    "\n",
    "Below is a simple example of tokenization of two sentences that have some words in common:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          |              |        |      |         |          |            |            |\n",
    "|----------|:------------:|:------:|:----:|:-------:|:--------:|:----------:|:----------:|\n",
    "| Text #1  |      Natural     | Language |  Processing | is | super | fun |            |\n",
    "| Index #1 |      1      |   2   |  3  |    4   |    5    |     6     |            |\n",
    "| Text #2  | Tokenization |   is   | used |    in   |  Natural |  Language  | Processing |\n",
    "| Index #2 |      7      |   4   |  8  |    9   |    1    |     2     |     3     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short example, it can be seen that the words \"Natural\", \"Language\", \"Processing\", and \"is\" have the same index in the two sentences, while the other words are assigned with other integers for their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00008-4c1b5d62-c0f1-4eb1-85a4-0f3c049dd57f",
    "deepnote_cell_type": "code",
    "id": "PRnDnCW-Z7qv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 5000\n",
      "There are 10663 unique words in total\n"
     ]
    }
   ],
   "source": [
    "# Create an instance object of the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = pd.read_csv('../../Data/sample2_under_20.csv')\n",
    "print(\"Number of sentences in the dataset:\", data.shape[0])\n",
    "\n",
    "corpus = data['0'].tolist()\n",
    "del data\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# To define the total number of words for machine processing, 1 is \n",
    "# added to the word index of the tokenizer created for the text \n",
    "# due to the inclusion of '0' used in padding\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print('There are %d unique words in total' %(total_words-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00009-0fc14628-ad10-4968-8427-ab6d482bd98b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-1f60c605-147a-43cb-97a4-05738dae92df",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## II. n-gram and Padding\n",
    "As the model is intended to generate text, it's essentially performing predictions on a word-by-word basis, meaning that it should be able to predict the next word based on the previous words. Therefore, the concept of **n-gram** is involved in the preprocessing, which is basically taking subsets of different lengths of the same sentences so that they can be later treated as different training instances.\n",
    "\n",
    "For example, for the sentence \"NLP stands for Natural Language Processing,\" it will be divided into the following subsets (the following table is using actual words to better illustrate, while the machine will perform this step using their word indices):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     |        |     |         |          |            |\n",
    "|:---:|:------:|:---:|:-------:|:--------:|:----------:|\n",
    "| Natural | Language |     |         |          |            |\n",
    "| Natural | Language | Processing |         |          |            |\n",
    "| Natural | Language | Processing | is |          |            |\n",
    "| Natural | Language | Processing | is | super |            |\n",
    "| Natural | Language | Processing | is | super | fun |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, these subsets will be later divided into predictors (the first until the second-to-last words) and target outputs (the last words) for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding** is a way to standardise the lengths of the text, after the masking is completed with the tokenizer above. \n",
    "Basically, it just adds as many 0s as needed to the end (or front) of each text, to match the longest sentence.\n",
    "\n",
    "Below is a simple example of padding using the two sentences above. As the second sentence is the longer one, only the first sentence will need padding, and only one 0 will be enough for its padding. This 0 can be added to the end or the front of text #1, and in this case, it's added to the front of the sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          |              |        |      |         |          |            |            |\n",
    "|----------|:------------:|:------:|:----:|:-------:|:--------:|:----------:|:----------:|\n",
    "| Text #1  |      Natural     | Language |  Processing | is | super | fun |            |\n",
    "| Index #1 |      1      |   2   |  3  |    4   |    5    |     6     |            |\n",
    "| Padded text #1 | 0      |   1   |  2  |    3   |    4  |    5     |    6       |\n",
    "| Text #2  | Tokenization |   is   | used |    in   |  Natural |  Language  | Processing |\n",
    "| Index #2 |      7      |   4   |  8  |    9   |    1    |    2     |     3    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00011-165ef40f-1c57-481e-8638-db3a7cba172a",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence has 35 words\n"
     ]
    }
   ],
   "source": [
    "input_sequences = []\n",
    "for line in corpus: # iterate through the list of sentences\n",
    "    # For each sentence, use the tokenizer to encode it into \n",
    "    # a sequence of numbers (word indices)\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    for i in range(1, len(token_list)): # iterate through the word \n",
    "                                        # indices of the sentence\n",
    "        n_gram_sequence = token_list[:i+1] # create subset of the sentence\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Determine the largest length of all sentences\n",
    "max_sequence_len = max([len(x) for x in input_sequences]) \n",
    "print('The longest sentence has %d words' %max_sequence_len)\n",
    "\n",
    "# Pad sequences (to the front)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                         maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create predictors and labels\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# One-hot encode the labels (target output)\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-fc750cb6-cc4a-43c3-b9b9-dcbd934f61bf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## III. Other preparations: functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00013-e00f846a-1b96-430a-8d92-2f0665825444",
    "deepnote_cell_type": "code",
    "id": "3YXGelKThoTT"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy/loss graphs after model training finishes\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.title(string + \" changes over epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00015-08ec03e9-5c9c-4882-b4c7-e77e38b6aa31",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def text_generator(seed_text, next_words, tokenizer, \n",
    "                   model, max_sequence_len):\n",
    "    '''\n",
    "    Returns a block of text generated using a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "        seed_text (str): several words to start the block of text\n",
    "        next_words (int): number of generated words desired\n",
    "        tokenizer: tokenizer of the trained model\n",
    "        model: trained model for text generation\n",
    "        max_sequence_len: the biggest length of the sentences fed to the model\n",
    "    '''\n",
    "    for w in range(next_words):\n",
    "        # Tokenize the previous words (i.e. the starting words when the \n",
    "        # text generation first begins)\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Pad sequences\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, \n",
    "                                   padding='pre')\n",
    "        # Generate model predictions based on the padded sequences\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            # Look for the corresponding word of the predicted word index \n",
    "            # output by the model above\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        # Add the predicted word\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-485d83b2-4c5f-4e5a-908b-637e9f829f73",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## IV. Model Training\n",
    "### i. Word Embedding\n",
    "After different words are encoded using word indices (tokenization), the machine is able to process them. However, another issue remains that the machine is unable to capture the meanings. Therefore, the method of **embedding** is used to represent each word as a vector so that words having similar meanings can be represented as vectors close to each other in the vector space. \n",
    "\n",
    "### ii. Recurrent Neural Network (RNN)\n",
    "When encountered with data involving \"contexts\", such as time-series data or language-related data, traditional neural networks are usually very ineffective, for they are unable to utilize the previous information for the next prediction. In comparison, **Recurrent Neural Network** proves to be much more effective in training these types of data, for it can additionally \"memorize\" the previous information and use it later. Essentially, RNN maintains an internal state about the steps it has seen so far, which enables it to retain some 'context' that is useful for time-series and Natural Language Processing. \n",
    "\n",
    "**Long Short Term Memory networks (LSTM)** are a type of RNN which tries to manage short and long term memory efficiently, by forgetting unimportant events after certain periods. LSTM also has further variations, one of which, called **Bidirectional LSTM**, is also used in this project. Bidirectionality refers to a model which runs on both the as-is as well as reversed versions of the sequence. \n",
    "\n",
    "For more details regarding RNN and LSTM, one can refer to resources such as https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "### Notes:\n",
    "For simplicity, only two models are shown in this notebook, the first one being the initial model and the second being the final model after several tunings. Both models involve using an Embedding layer in the beginning and a Dense layer with the softmax activation function in the end. The tunings (the detailed steps are not shown in here) involve adjusting:\n",
    "- the number of *layers in between and their node numbers*\n",
    "- the usage of *Dropout layers*\n",
    "- the usage of *regularization*\n",
    "- the type of *optimizer*\n",
    "- the value of *learning rate*\n",
    "\n",
    "### (1) Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00018-46e544d9-7bdd-460b-9689-e04363a4a228",
    "deepnote_cell_type": "code",
    "id": "soPGVheskaQP",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1214/1214 [==============================] - 47s 29ms/step - loss: 8.2462 - accuracy: 0.0298\n",
      "\n",
      "Epoch 00001: loss improved from inf to 8.24624, saving model to model_weights2.hdf5\n",
      "Epoch 2/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 9.4944 - accuracy: 0.0288\n",
      "\n",
      "Epoch 00002: loss did not improve from 8.24624\n",
      "Epoch 3/30\n",
      "1214/1214 [==============================] - 37s 31ms/step - loss: 9.4619 - accuracy: 0.0332\n",
      "\n",
      "Epoch 00003: loss did not improve from 8.24624\n",
      "Epoch 4/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 8.8561 - accuracy: 0.03760s - los\n",
      "\n",
      "Epoch 00004: loss did not improve from 8.24624\n",
      "Epoch 5/30\n",
      "1214/1214 [==============================] - 36s 29ms/step - loss: 8.3087 - accuracy: 0.0438\n",
      "\n",
      "Epoch 00005: loss did not improve from 8.24624\n",
      "Epoch 6/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 7.9091 - accuracy: 0.0471\n",
      "\n",
      "Epoch 00006: loss improved from 8.24624 to 7.90909, saving model to model_weights2.hdf5\n",
      "Epoch 7/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 7.5519 - accuracy: 0.05570s - loss: 7\n",
      "\n",
      "Epoch 00007: loss improved from 7.90909 to 7.55187, saving model to model_weights2.hdf5\n",
      "Epoch 8/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 7.0944 - accuracy: 0.0638\n",
      "\n",
      "Epoch 00008: loss improved from 7.55187 to 7.09444, saving model to model_weights2.hdf5\n",
      "Epoch 9/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.8023 - accuracy: 0.0688\n",
      "\n",
      "Epoch 00009: loss improved from 7.09444 to 6.80227, saving model to model_weights2.hdf5\n",
      "Epoch 10/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.5968 - accuracy: 0.07190s - loss: 6.5971 - accuracy: 0.07 - ETA: 0s - loss: 6.5968 - accuracy: \n",
      "\n",
      "Epoch 00010: loss improved from 6.80227 to 6.59679, saving model to model_weights2.hdf5\n",
      "Epoch 11/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.5149 - accuracy: 0.0737\n",
      "\n",
      "Epoch 00011: loss improved from 6.59679 to 6.51490, saving model to model_weights2.hdf5\n",
      "Epoch 12/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.4628 - accuracy: 0.0753\n",
      "\n",
      "Epoch 00012: loss improved from 6.51490 to 6.46283, saving model to model_weights2.hdf5\n",
      "Epoch 13/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.4061 - accuracy: 0.0755\n",
      "\n",
      "Epoch 00013: loss improved from 6.46283 to 6.40605, saving model to model_weights2.hdf5\n",
      "Epoch 14/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.3394 - accuracy: 0.0776\n",
      "\n",
      "Epoch 00014: loss improved from 6.40605 to 6.33936, saving model to model_weights2.hdf5\n",
      "Epoch 15/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.2762 - accuracy: 0.0794\n",
      "\n",
      "Epoch 00015: loss improved from 6.33936 to 6.27621, saving model to model_weights2.hdf5\n",
      "Epoch 16/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.2309 - accuracy: 0.0799\n",
      "\n",
      "Epoch 00016: loss improved from 6.27621 to 6.23095, saving model to model_weights2.hdf5\n",
      "Epoch 17/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.2009 - accuracy: 0.0800\n",
      "\n",
      "Epoch 00017: loss improved from 6.23095 to 6.20088, saving model to model_weights2.hdf5\n",
      "Epoch 18/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.1726 - accuracy: 0.0817\n",
      "\n",
      "Epoch 00018: loss improved from 6.20088 to 6.17264, saving model to model_weights2.hdf5\n",
      "Epoch 19/30\n",
      "1214/1214 [==============================] - 37s 31ms/step - loss: 6.1419 - accuracy: 0.0830\n",
      "\n",
      "Epoch 00019: loss improved from 6.17264 to 6.14189, saving model to model_weights2.hdf5\n",
      "Epoch 20/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.1134 - accuracy: 0.08160s\n",
      "\n",
      "Epoch 00020: loss improved from 6.14189 to 6.11341, saving model to model_weights2.hdf5\n",
      "Epoch 21/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.1071 - accuracy: 0.0820\n",
      "\n",
      "Epoch 00021: loss improved from 6.11341 to 6.10710, saving model to model_weights2.hdf5\n",
      "Epoch 22/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.0890 - accuracy: 0.0842\n",
      "\n",
      "Epoch 00022: loss improved from 6.10710 to 6.08905, saving model to model_weights2.hdf5\n",
      "Epoch 23/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.0751 - accuracy: 0.0841\n",
      "\n",
      "Epoch 00023: loss improved from 6.08905 to 6.07506, saving model to model_weights2.hdf5\n",
      "Epoch 24/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 6.0610 - accuracy: 0.0841\n",
      "\n",
      "Epoch 00024: loss improved from 6.07506 to 6.06103, saving model to model_weights2.hdf5\n",
      "Epoch 25/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.0359 - accuracy: 0.0838\n",
      "\n",
      "Epoch 00025: loss improved from 6.06103 to 6.03595, saving model to model_weights2.hdf5\n",
      "Epoch 26/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 6.0242 - accuracy: 0.0839\n",
      "\n",
      "Epoch 00026: loss improved from 6.03595 to 6.02418, saving model to model_weights2.hdf5\n",
      "Epoch 27/30\n",
      "1214/1214 [==============================] - 36s 30ms/step - loss: 5.9953 - accuracy: 0.0854\n",
      "\n",
      "Epoch 00027: loss improved from 6.02418 to 5.99527, saving model to model_weights2.hdf5\n",
      "Epoch 28/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 5.9876 - accuracy: 0.0858\n",
      "\n",
      "Epoch 00028: loss improved from 5.99527 to 5.98758, saving model to model_weights2.hdf5\n",
      "Epoch 29/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 5.9740 - accuracy: 0.0850\n",
      "\n",
      "Epoch 00029: loss improved from 5.98758 to 5.97399, saving model to model_weights2.hdf5\n",
      "Epoch 30/30\n",
      "1214/1214 [==============================] - 37s 30ms/step - loss: 5.9543 - accuracy: 0.0865\n",
      "\n",
      "Epoch 00030: loss improved from 5.97399 to 5.95434, saving model to model_weights2.hdf5\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x0000027B9996AF10>\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 34, 150)           1599600   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 34, 400)           561600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 34, 400)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10664)             4276264   \n",
      "=================================================================\n",
      "Total params: 7,399,064\n",
      "Trainable params: 7,399,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construct the initial model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 150, # the vectors representing words have\n",
    "                                      # a length of 150\n",
    "                    input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(200)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Save the model\n",
    "filepath = \"model_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(xs, ys, epochs=30, batch_size=64, verbose=1, \n",
    "                    callbacks=desired_callbacks)\n",
    "\n",
    "print(model)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00019-14e88b0f-4a1c-474e-bee2-5e2e8b6b8bfe",
    "deepnote_cell_type": "code",
    "id": "poeprYK8h-c7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuEUlEQVR4nO3deXxV9Z3/8dcn+072QICEVWQRrCJFWxXXutZpp4ttHavOjKXTztiZ6fbrTKedzvKrnc6i06kMre3U2tqfVdvauhS1RW0VERkQEBFI2ANJCCEkIWT7/P64J3gJSUggJzc39/18PM7jnnvOued8Tg7czz3f7/d8v+buiIhIYkuKdQAiIhJ7SgYiIqJkICIiSgYiIoKSgYiIoGQgIiIoGcgAzGyHmV0Z4ximmJmbWUos45DBMbOVZvYnsY5Dhk7JQERElAxExgLdOcmZUjKQQTGzdDP7DzPbF0z/YWbpwbpiM/uVmTWaWYOZvWhmScG6L5jZXjM7YmZbzOyKfvafaWb/amY7zeywmf3OzDKjNvmYme0ys3oz+5uozy0ys5eDY9eY2bfMLC1qvZvZUjPbamaHzOy/zMyCdcnBMevNrNrMPh1dJGVm48zs/mC/e83sH80sOVg3w8yeD2KtN7P/N8Df7r1mtimIcaWZzQ6Wf9HMHum17T1mdu8gjn+bmf3ezP7dzBqAr/Zx3KTgGNvN7KCZPWxmhcG6nuK3O4PrWWNmfz2Y6x2sv8nM1plZU7D/a6IOXRnEdsTMVphZcfCZDDN7MIil0cxeNbOy/v5uMsLcXZOmPidgB3BlMP81YBVQCpQALwH/EKz7v8AyIDWYLgYMmAXsBsqD7aYA0/s51n8BK4GJQDJwEZAefMaB7wCZwALgGDA7+Nz5wGIgJdh2M/CZqP068CsgH6gA6oBrgnVLgTeASUAB8GywfUqw/ufAfwPZwXmvBj4RrHsI+BsiP6gygHf3c15nAS3AVcHf5vPANiANqARagbxg22SgBlg8iOPfBnQCfx6ce2Yfx/5McM0mBX/L/wYeiroWHpxHNnBO8LcZzPVeBBwOzikpuGZnB+tWAtuD884M3n89WPcJ4JdAVnCu5/ecu6bYTzEPQNPonTgxGWwHrota9x5gRzD/NeAXwIxen58B1AJXAqkDHCcJOAos6GNdz5fWpKhlq4Gb+9nXZ4CfRb336C9q4GHgi8H8b3q+XIP3VwbbpwBlRJJOZtT6jwC/DeYfAJZHx9VPPF8GHu51rnuBJcH73wG3BvNXAduD+VMd/zZg1ymOvRm4Iur9BKCDtxOn93yJB+u/Adw/iOv938C/93PMlcDfRr3/M+DpYP4OIkllfqz/bWs6eVIxkQxWObAz6v3OYBnAvxD5tbvCzKrM7IsA7r6NyJfzV4FaM/uJmZVzsmIiv663D3D8/VHzrUAOgJmdFRRR7TezJuCfg/2d8rNB/Luj1kXPVxL5JV8TFGk0EvkSLA3Wf57I3c/qoAjojn7iPuHv5u7dwXEmBot+TORLHuCjwfvBHL93vH2pBH4W9fnNQBeRRNPXPqKv6UDXezKnca2AHwK/Bn4SFD19w8xST3EOMkKUDGSw9hH5culRESzD3Y+4+1+7+zTgRuCveuoG3P3H7v7u4LMO3N3HvuuBNmD6acR1H/AmMNPd84AvEfmSHowaIkUoPSZHze8m8su82N3zgynP3ecCuPt+d/9Tdy8nUvzxbTOb0ccxTvi7BfUVk4ncHQD8FFhiZpOA9/F2Mhjw+IFTdTm8G7g26vP57p7h7nujtok+5+PXtHfcvdbt5jSulbt3uPvfu/scIsWANwC3DnU/Eg4lAxmsh4C/NbOSoELw74AHAczshqBC1YAmIr8+u8xslpldHlQ8thEpCurqvePg1/L3gH8zs/KgYvfC6ArLAeQGx2w2s7OBTw7hnB4G7jKziWaWD3whKqYaYAXwr2aWF1TGTjezS4Nz/mDwBQ5wiMgX80nnFhzjejO7IvgV/NdEvuRfCo5TR6Ro5ftAtbtvHszxB2kZ8E9mVhnEXGJmN/Xa5stmlmVmc4HbgZ6K8H6vN3A/cHtwTknB3+/sUwVjZpeZ2TlBJXgTkSKrvv5mEgNKBjJY/wisAV4HNgBrg2UAM4lUvjYDLwPfdveVRCotv07kl/9+IkUcX+pn/58N9vsq0EDkDmIw/z4/S6R45QiRSuZ+W/X04TtEvnBfB/4XeJJIpWzPF9StRCp63yDyhf8IkXJ3gAuAV8ysGXgcuMvdq3sfwN23ALcA/0nk73AjcKO7t0dt9mMi9RU/7vXxgY4/GPcEsa0wsyNEKoTf2Wub54kU8T0HfNPdVwTL+73e7r6aSOL4dyIVyc9z4l1Ef8YH59BEpMjqed5OMBJj5q7BbUQAzOxaYJm7D+aLLa6Z2RSgmkjFfmeMw5FRQHcGkrAs8mzDdWaWYmYTga8AP4t1XCKxoGQgicyAvydSBPO/RIou/i6mEYnEiIqJREREdwYiIhJ5EjGuFBcX+5QpU2IdhohIXHnttdfq3b2kv/VxlwymTJnCmjVrYh2GiEhcMbOdA61XMZGIiCgZiIiIkoGIiKBkICIiKBmIiAghJwMzu8vMNgb9vX+mj/VLLDJs4Lpg0tOfIiIxEFrTUjObB/wpkSHy2oGnzewJd9/aa9MX3f2GsOIQEZFTC/POYDawyt1bg14RnycyeMeo97ut9Tzw8g5WVR3kUEv7qT8gIhLnwnzobCORgTWKiAxqch2R/tF7u9DM1hMZRemz7r6p9wZmdidwJ0BFRUV4EQc+/8h69h1uO/6+NDedWeNzOassl1njc5lVlsvMshyy0uLumT0RkT6F9m3m7pvN7G7gGSKDnqwnMnBItLVApbs3m9l1wM+JDJTSe1/LiQw+zsKFC0PtWa+1vZN9h9v4k3dP5eKzSnhr/xHe3H+Etw4c4Uev7KStoxsAM6gozGLBpHzu/sP5ZKYlhxmWiEioQv1p6+73ExkiDzP7Z2BPr/VNUfNPmtm3zazY3evDjGsg1fUtAJxXWcClZ5Vw6Vlvd+XR1e3sbmg9nhxe39PI4+v38Z6547l+/lAGoBIRGV1CTQZmVurutWZWAbwfuLDX+vHAAXd3M1tEpA7jYJgxnUpVXSQZTC3OPmldcpIxpTibKcXZXDNvPJ1d3VzwT8/yzBv7lQxEJK6FXej9aFBn0AF8yt0PmdlSAHdfBnwA+KSZdRKpV7jZYzzAQs+dQV/JoLeU5CSumF3Gik376ejqJjVZj22ISHwKu5jo4j6WLYua/xbwrTBjGKqqumYm5meSkTq4OoCr5pTxyGt7WF3dwLtmFIccnYhIOPRTtpfq+hamlZz6rqDHJTNLyEhN4pk3DoQYlYhIuJQMorg7VXUtgyoi6pGZlsy7Z5SwYtN+NISoiMQrJYMo9c3tHDnWybQhJAOAq+eUse9wG5v2NZ16YxGRUUjJIEpVXTMA00pyhvS5K2aXkmSwQkVFIhKnlAyiDKUlUbSinHTOryxQvYGIxC0lgyhV9S2kpSQxMT9zyJ+9es54Ntc0sbuhNYTIRETCpWQQpaquhalF2SQl2ZA/e9WcMgDdHYhIXFIyiFJV3zykZqXRphRnc1ZZjpKBiMQlJYNAR1c3uw62Drm+INpVc8pYvaOBxlZ1ey0i8UXJILDn0FE6u33ILYmiXTVnPF3dzm/erB3GyEREwqdkEOhpVnomdwbzJ46jLC+dFZtUVCQi8UXJINDTrHT6adYZACQlGVfOLuOFrXW0dXQNV2giIqFTMghsr2uhICuV/Ky0M9rP1XPH09rexe+3xWxIBhGRIVMyCFTXN59RfUGPxdMKyUlPUasiEYkrSgaBqrqWIfdJ1Jf0lGSWzCrh2c0H6OpWx3UiEh+UDIDmY53UHjnG1DOoL4h21Zwy6pvbWbf70LDsT0QkbEoGQHUw1OW04jMvJgK47OxSUpNNHdeJSNxQMiDy5DFw2k8f95aXkcriaUU8oyamIhInlAyI1BeYQWVR1rDt86o5ZVTVt7CttnnY9ikiEpZQk4GZ3WVmG81sk5l9po/1Zmb3mtk2M3vdzM4LM57+VNW3MKkgk/SUwY17PBhXzo50XLfijf3Dtk8RkbCElgzMbB7wp8AiYAFwg5nN7LXZtcDMYLoTuC+seAZSXd88bPUFPcrzMzln4jg1MRWRuBDmncFsYJW7t7p7J/A88L5e29wEPOARq4B8M5sQYkwncXeqhzju8WBdPaeMdbsbqW1qG/Z9i4gMpzCTwUbgEjMrMrMs4Dpgcq9tJgK7o97vCZadwMzuNLM1Zramrq5uWIOsPXKMlvauM+qGoj9XzS3DHZ7drI7rRGR0Cy0ZuPtm4G7gGeBpYD3Q2WuzvkaROelJLXdf7u4L3X1hSUnJsMa5/XgHdcNbTAQwqyyXisIsnlG9gYiMcqFWILv7/e5+nrtfAjQAW3ttsocT7xYmAfvCjKm3ng7qhqtZaTQz46o5Zfx+20Gaj/XOgyIio0fYrYlKg9cK4P3AQ702eRy4NWhVtBg47O41YcbUW1VdC5mpyYzPywhl/1fNKaO9q5sX3hre4i0RkeGUEvL+HzWzIqAD+JS7HzKzpQDuvgx4kkhdwjagFbg95HhOUl3fwpTi0xv3eDAWVhZQkJXKik37ue6cEa0bFxEZtFCTgbtf3MeyZVHzDnwqzBhOpaqumbkTx4W2/5TkJC4/u4xn3thPR1c3qcl6zk9ERp+E/mZq7+xm96Gjw9Jb6UCunltGU1snq6sbQj2OiMjpSuhksKuhla5uD6XyONrFM4tJT0lixSa1KhKR0Smhk0FViM1Ko2WlpbBkVglPbdyvMQ5EZFRK6GTQ06w0jKePe7txQTm1R46pqEhERqWETgZVdS0U56QxLjM19GNdfnYpWWnJ/PL1EX2MQkRkUBI6GVTXtwx7B3X9yUpL4crZZTy1oYaOru4ROaaIyGAldDKoqm8ekSKiHjcuKOdQawe/31Y/YscUERmMhE0Gh492UN/cHnpLomiXnFVMbkYKv1w/og9Zi4icUsImg7f7JBqZYiKA9JRkrpk7nhWb9tPW0TVixxUROZUETgY9zUpH7s4A4IYF5Rw51snz6qtIREaRhE0GVXUtJCcZFYXDN+7xYFw0vYjC7DR+9bqKikRk9EjoZDC5IJO0lJH9E6QmJ3HtvPE8+8YBWtvVrbWIjA6JmwzqW0a0viDajQvKOdrRxXMaAU1ERomETAbd3U71CDcrjXbBlELK8tL55Xo9gCYio0NCJoP9TW20dXSPaLPSaMlJxvXnlLNySx1NbR0xiUFEJFpCJoOqupHrk6g/Ny6YQHtXNys2HYhZDCIiPRIyGfQ0K50eozoDgHMn5zOpIFNFRSIyKiRkMthe10J2WjKluekxi8HMuHFBOb/bVk9DS3vM4hARgQRNBtX1LUwtycYsnHGPB+vG+eV0dTtPb9SgNyISW6EmAzP7SzPbZGYbzewhM8votX6JmR02s3XB9HdhxtOjqr55xHorHcjsCblML8lWUZGIxFxoycDMJgJ/ASx093lAMnBzH5u+6O7nBtPXwoqnx7HOLvYcOhrTyuMeZsYN88tZVX2Q2qa2WIcjIgks7GKiFCDTzFKALCDmP4F3HmzFnZg1K+3txgUTcIcnNqh7ChGJndCSgbvvBb4J7AJqgMPuvqKPTS80s/Vm9pSZze1rX2Z2p5mtMbM1dXVn1sFbz7jHo6GYCGBGaS6zJ+SpqEhEYirMYqIC4CZgKlAOZJvZLb02WwtUuvsC4D+Bn/e1L3df7u4L3X1hSUnJGcVV1TPu8Si5M4DI3cHaXY3sbmiNdSgikqDCLCa6Eqh29zp37wAeAy6K3sDdm9y9OZh/Ekg1s+IQY6KqroXS3HRy0lPCPMyQ3Di/HFBRkYjETpjJYBew2MyyLNKG8wpgc/QGZjY+WIeZLQriORhiTJFxj0fRXQHA5MIszp2cr6IiEYmZMOsMXgEeIVIUtCE41nIzW2pmS4PNPgBsNLP1wL3Aze7uYcUEkTqDqaOkviDajQvK2bSv6XidhojISAq1NZG7f8Xdz3b3ee7+R+5+zN2XufuyYP233H2uuy9w98Xu/lKY8RxqaedQawfTR9mdAcD150zADA16IyIxkVBPIFcdH/d49CWD8eMyWDSlkMfX7yPkmyMRkZMkVDKo7mlJNAqLiSBSVLSttpktB47EOhQRSTAJlQyq6ppJSTImF2TGOpQ+XTtvPMlJpopkERlxCZUMqutbqCjKIiV5dJ52UU46F00v4pfra1RUJCIjanR+K4akqq5l1Dx53J/3LihnV0MrL1eF2sJWROQECZMMurqd6oOj7xmD3m5cUE5xThrLnq+KdSgikkASJhnsazxKe2c300ZBb6UDyUhN5o53T+WFt+rYuPdwrMMRkQSRMMngeJ9EozwZANyyuJLc9BTue357rEMRkQSRMMkgMzWZy2aVML10dNcZAORlpHLLhZU8taHmeHNYEZEwJUwyWDS1kO/fvojinNiNezwUt79rCinJSSx/QXcHIhK+hEkG8aY0N4MPnj+JR1/bywGNgiYiIVMyGMU+ccl0Oru7uf931bEORUTGOCWDUayiKIsb5pfzo1U7OdzaEetwRGQMUzIY5T65ZDot7V088PKOWIciImOYksEoN3tCHpfNKuH7L+3gaHtXrMMRkTFKySAO/NllM2hoaefhNbtjHYqIjFFKBnHggimFLKwsYPkLVXR0dcc6HBEZg5QM4sQnl0xnb+NRdW8tIqFQMogTl59dyqyyXO5buZ3ubnVvLSLDK9RkYGZ/aWabzGyjmT1kZhm91puZ3Wtm28zsdTM7L8x44pmZ8ckl09la28xzb9bGOhwRGWNCSwZmNhH4C2Chu88DkoGbe212LTAzmO4E7gsrnrHghvkTmFSQybdXbtPgNyIyrMIuJkoBMs0sBcgCehd43wQ84BGrgHwzmxByTHErJTmJT1wyjf/d1cgr1Q2xDkdExpDQkoG77wW+CewCaoDD7r6i12YTgej2knuCZScwszvNbI2Zramrqwsr5LjwwYWTKc5J476V6sBORIZPmMVEBUR++U8FyoFsM7ul92Z9fPSk8g93X+7uC919YUlJyfAHG0cyUpO5/V1TeV6D34jIMAqzmOhKoNrd69y9A3gMuKjXNnuAyVHvJ3FyUZL0csviSnLSU1imwW9EZJiEmQx2AYvNLMvMDLgC2Nxrm8eBW4NWRYuJFCXVhBjTmDAuM5VbFlfy5IYadmjwGxEZBmHWGbwCPAKsBTYEx1puZkvNbGmw2ZNAFbAN+A7wZ2HFM9bc8e7I4DeqOxCR4ZAS5s7d/SvAV3otXha13oFPhRnDWFWam8FHF1Xww1U7+cSl05hWMvqH8xSR0UtPIMexT102g/SUJP7tmbdiHYqIxDklgzhWkpvOHe+ayq9er2HTPrUsEpHTp2QQ5/70kmmMy0zlm7/eEutQRCSODSoZmNldZpYXtPq538zWmtnVYQcnpzYuM5Wll07nt1vqeHWHnkoWkdMz2DuDO9y9CbgaKAFuB74eWlQyJLddNIXS3HS+8fSb6rNIRE7LYJNBz5PC1wHfd/f19P30sMRAZloyf37FTF7dcYiVbyV2dx0icnoGmwxeM7MVRJLBr80sF9CQW6PIhxdOZnJhJv/y9BaNdyAiQzbYZPDHwBeBC9y9FUglUlQko0RaShJ/ddVZvFHTxBMb9BC3iAzNYJPBhcAWd28MOpv7W0BtGUeZ9y6YyKyyXP7tmbfo1FjJIjIEg00G9wGtZrYA+DywE3ggtKjktCQnGZ99zyyq61t45LU9sQ5HROLIYJNBZ9B1xE3APe5+D5AbXlhyuq6cXco7KvK557mttHV0xTocEYkTg00GR8zs/wB/BDxhZslE6g1klDEzPveeWdQcbuPBVTtjHY6IxInBJoMPA8eIPG+wn8hoZP8SWlRyRi6aXszFM4v59srtNB/rjHU4IhIHBpUMggTwI2Ccmd0AtLm76gxGsc9ePYuGlna++2JVrEMRkTgw2O4oPgSsBj4IfAh4xcw+EGZgcmYWTM7nmrnj+e6L1TS0tMc6HBEZ5QZbTPQ3RJ4x+Li73wosAr4cXlgyHD77nrNobe/kvpXbYh2KiIxyg00GSe5eG/X+4BA+KzEyozSX9583iR+8vJOaw0djHY6IjGKD/UJ/2sx+bWa3mdltwBNEhqyUUe6uK2bi7tz73NZYhyIio9hgK5A/BywH5gMLgOXu/oWBPmNms8xsXdTUZGaf6bXNEjM7HLXN353meUg/Jhdm8bF3VvLwmj3sbmiNdTgiMkoNegxkd38UeHQI228BzgUInkvYC/ysj01fdPcbBrtfGbo7L5nGAy/v4KHVu/j8NWfHOhwRGYUGvDMwsyPBL/re0xEzaxrCca4Atru7noKKgfL8TK6YXcbDa3bT3qk+i0TkZAMmA3fPdfe8PqZcd88bwnFuBh7qZ92FZrbezJ4ys7lD2KcMwS2LK6lvbufpTftjHYqIjEKhtwgyszTgvcBP+1i9Fqh09wXAfwI/72cfd5rZGjNbU1enwVtOx8UziqksyuLBl3VzJiInG4nmodcCa939QO8V7t7k7s3B/JNAqpkV97Hdcndf6O4LS0pKwo94DEpKMj66qILVOxrYsv9IrMMRkVFmJJLBR+iniMjMxpuZBfOLgngOjkBMCemDCyeTlpLEj17R3YGInCjUZGBmWcBVwGNRy5aa2dLg7QeAjWa2HrgXuNk1ontoCrPTuOGcCTy2di8t6sBORKKEmgzcvdXdi9z9cNSyZe6+LJj/lrvPdfcF7r7Y3V8KMx6Bjy2upPlYJ79Yty/WoYjIKKIuJRLMeRX5zJ6Qxw9X7UQ3YSLSQ8kgwZgZtyyuYHNNE2t3NcY6HBEZJZQMEtAfnDuRnPQUfqSR0EQkoGSQgLLTU3j/eRP51YYajXUgIoCSQcK6ZXEl7Z3d/HTN7liHIiKjgJJBgjqrLJdFUwr58epddHerIlkk0SkZJLCPLa5g58FWXtxWH+tQRCTGlAwS2DXzxlOUncaDqkgWSXhKBgksPSWZD10wmec2H2Bfo4bFFElkSgYJ7qOLKnDgodW7Yh2KiMSQkkGCm1yYxWWzSvnJq7vp6NLANyKJSslAuGVxBXVHjrFi00m9jItIglAyEC49q5SJ+ZmqSBZJYEoGQnKS8dF3VvBy1UG21WrgG5FEpGQgAHz4gsmkJhsPrlJFskgiUjIQAIpz0rl23gQeXbuH1nYNfCOSaJQM5LhbFldypK2TxzXwjUjCUTKQ4y6YUsCcCXnc89xWjrR1xDocERlBSgZynJnxD38wj/1NbfzLr7fEOhwRGUGhJQMzm2Vm66KmJjP7TK9tzMzuNbNtZva6mZ0XVjwyOOdXFvDxC6fww1U7WbOjIdbhiMgICS0ZuPsWdz/X3c8FzgdagZ/12uxaYGYw3QncF1Y8Mnife88sysdl8oVHX6etoyvW4YjICBipYqIrgO3u3vupppuABzxiFZBvZhNGKCbpR3Z6Cv/0vnlsr2vh27/dFutwRGQEjFQyuBl4qI/lE4Hoobb2BMskxpbMKuV975jIt1du5839TbEOR0RCFnoyMLM04L3AT/ta3ceyk4bdMrM7zWyNma2pq6sb7hClH1++YQ55mal84dENdGk0NJExbSTuDK4F1rp7X72g7QEmR72fBJzUyN3dl7v7QndfWFJSElKY0lthdhpfuXEO63c38j8v7Yh1OCISopFIBh+h7yIigMeBW4NWRYuBw+5eMwIxySC9d0E5l59dyjd/vYXdDa2xDkdEQhJqMjCzLOAq4LGoZUvNbGnw9kmgCtgGfAf4szDjkaHrefYgyeBLP9uAu4qLRMaiUJOBu7e6e5G7H45atszdlwXz7u6fcvfp7n6Ou68JMx45PRPzM/nCtWfz4tZ6Hlu7N9bhiEgI9ASyDMot76zk/MoC/uGJN6hvPhbrcERkmCkZyKAkJRl3/+E5tB7r4quPb4p1OCIyzJQMZNBmlOby6ctn8KvXa3j2DQ2RKTKWKBnIkCy9dDqzynL58i82qmdTkTFEyUCGJC0libs/MJ/9TW1842n1bCoyVigZyJCdOzmf2y+ayg9X7eTx9RoIR2QsUDKQ0/L5a2axaGohf/3wOl54S12EiMQ7JQM5LRmpyXz34wuZUZrL0gdfY93uxliHJCJnQMlATlteRio/uOMCinPSuf37q9lWeyTWIYnIaVIykDNSmpvBD/94EclJSdx6/2r2NR6NdUgichqUDOSMVRZl84M7LuBIWye3fm81h1raYx2SiAyRkoEMi7nl4/jOxxeyq6GV2//nVVrbO2MdkogMgZKBDJvF04r4z4+8g9f3NLL0wbW0d3bHOiQRGSQlAxlW75k7nq+/fz4vvFXH5x5ZT7dGSBOJCymxDkDGng9dMJmDLe3c/fSbFGRFRksz62uEUxEZLZQMJBRLL53GweZjfPd31RTnpPHpy2fGOiQRGYCSgYTCzPjSdbNpaGnnmyve4khbJ3988VRKczNiHZqI9EHJQEKTlGTc/YH5YLD8xSq+9/tqbpxfzu3vmso5k8bFOjwRiWLxNqbtwoULfc0ajY4Zb3bUt/A/L+3gp2t209LexQVTCrj9XVO5ek4ZKclqxyASNjN7zd0X9rs+zGRgZvnAd4F5gAN3uPvLUeuXAL8AqoNFj7n71wbap5JBfGtq6+Cna/bwPy9Vs7vhKBPzM7n1wkpuvqCCcVmpsQ5PZMyKdTL4AfCiu3/XzNKALHdvjFq/BPisu98w2H0qGYwNXd3Oc5sP8L3fV7OqqoHM1GT+8PyJ3HbRFGaU5sY6PJEx51TJILQ6AzPLAy4BbgNw93ZA/RQIAMlJxtVzx3P13PG8sa+J7/++mofX7OHBVbuYWpzNxTOLuWRmCRdOLyI7XVVbImEL7c7AzM4FlgNvAAuA14C73L0lapslwKPAHmAfkbuEk0ZbN7M7gTsBKioqzt+5c2coMUts1Tcf41fr9/HC1npe3n6Qox1dpCYb51cWcMlZJVwys4Q5E/JIStIzCyJDFbNiIjNbCKwC3uXur5jZPUCTu385aps8oNvdm83sOuAedx+wQbqKiRLDsc4uXttxiOe31vHCW/VsrmkCoDgnjYtnlnDxzGIuPauEopz0GEcqEh9imQzGA6vcfUrw/mLgi+5+/QCf2QEsdPf6/rZRMkhMtU1tvLi1nhe21vHi1noaWtoxg/MqCrhidilXzi5jZmmOnnQW6UesK5BfBP7E3beY2VeBbHf/XNT68cABd3czWwQ8AlT6AEEpGUh3t7Nx32F+82Ytz22uZcPewwBUFGYdTwyLphaSqiarIsfFOhmcS6RpaRpQBdwOfBjA3ZeZ2aeBTwKdwFHgr9z9pYH2qWQgve0/3MZzbx7g2TcO8PvtB2nv7CY3I4VLzyrhytllLJlVQn5WWqzDFImpmCaDMCgZyEBa2zv53dZ6nt18gN+8WUt9czspScYlZ5Vw07nlXDWnjKw0tU6SxBOzpqUisZCVlnK8yWp3t7N+TyNPb9zP4+v38Zs3a8lKS+Y9c8fz3nPLuXhGsZ5+FgnozkASQne3s3pHA79Yt5cnXq+hqa2Touw0bpg/gZveMZF3TM5X5bOMaSomEunlWGcXK7fU8fi6fTy7+QDHOrupLMripgXlvPfccj0BLWOSkoHIAI60dfD0xv38Yt0+XtpeT7fDrLJcrp8/gevnT2B6SU6sQxQZFkoGIoNU29TGUxv388TrNby6swF3OHt8LjfMn8D188uZWpwd6xBFTpuSgchp2H+4jac21vDE6zWs2XkIgLnleZE7hnMmUFmkxCDxRclA5AzVHD7Kkxv288Tr+1i7qxGAORPyuHB6EYumFnLBlEIKs/Ucg4xuSgYiw2hv41Ge2lDDM28cYN3uRo51dgMwszSHRVMLj08TxmXGOFKREykZiITkWGcXG/Yc5pXqBl7d0cCaHYdoPtYJwOTCTC6YUsg7pxYyb+I4ppfkkJGaHOOIJZHpoTORkKSnJLNwSiELpxQCkQF7Ntc0sbq6gdXVDazcUsdja/cCYAaTCjKZUZLDjNKoqSRXI7zJqKBkIDJMkpOMeRPHMW/iOO5491Tcne11Lby5v4lttc3Hp57+k3oU56QzozSbGaU5TC3OYVpJNtOKs5lUkEWyxm6QEaJkIBISMzt+BxCtq9vZc6iV7XXNJySJx9fto6mt8/h2aclJVBRlMbU4+3iCmFaSw9TibIqy0/TEtAwrJQOREZacZFQWZVNZlM3lZ5cdX+7uNLS0U13fQlVdC1X1LVTVNVNd38LzW+po73r7bmJcZiozS3OYWZbDjNLc4/Pj8zKUJOS0KBmIjBJmRlFOOkU56cfrIXp0dTt7Dx1le30zVXUtkbuKA808tXE/ja27j2+Xk57CjNKcqESRw7TiHCYVZKpTPhmQkoFIHEhOMiqKsqgoyuKyWW8vd3cOtrSzrbaZrbXNbDtwhK21zax8q46fvrbn+HapyZG7kZ4ip+lB3cTU4mwKVeQkKBmIxDUzozgnneKcdBZPKzph3eHWDrbVHWF7XVDsNECR07SSbAqz0shITSY9NYn0lGQyol4zUpNJT4m8ZqUlU5KTTmleOiU5GeRlpiiZjAFKBiJj1LisVM6vLOT8yhOLnDq7utnbePR4cVNVfQvVdS3UHG7jWGcXbR3dHOvs4lhHN22dXXR0DfwsUlpKEqW56ZTkpke9ZlCam05hdhoF2WkUZKWSn5VGfmaqiqtGKSUDkQSTkpx0vAL7srNLT7l9V7efkCRajnVSe+QYdVFTz/vq+hZeqW6gsbWj3/3lZqRQkPV2gnj7NY3C7Lfn87NSjyeSzNRk3X2ETMlARAaUnGRkpaUQPYz0qcZ8ONbZRX1zO4da2jnU2s6h1g4aW9s51NLBodb2yHxrZL6qvpnGlg6OHOvsd39pKUkUZKVSkJXG+HEZVBZmMbkwi4rCSD3K5IIsstP1dXYmQv3rmVk+8F1gHuDAHe7+ctR6A+4BrgNagdvcfW2YMYlI+NJTkpmYn8nE/MH30dTR1U1jT9Jo7aCh5e2kEXltp6Glg32NR0/o+qNHcU46FYWZkQRRmMXEgkzGZaaSl5lKXkZqZD4jlZyMFD3M14ewU+k9wNPu/gEzSwOyeq2/FpgZTO8E7gteRSTBpCYnURLUOZyKu9PY2sGuhtbj0+7g9dUdh3h8/T66B6jqyE1PIS8zldyMyGv5uAyml+QwrSSH6aXZTCnKTri+pEJLBmaWB1wC3Abg7u1Ae6/NbgIe8EhveavMLN/MJrh7TVhxiUj8M7NIfUJ2Ggsm55+0vr2zm9ojbTQd7aSprYOmox00tXUGrx0nLG882sHq6gZ+vm5f1P5hYn5mkCCyj79WFGaRnZZCZlqkddVYqscI885gGlAHfN/MFgCvAXe5e0vUNhOB3VHv9wTLTkgGZnYncCdARUVFiCGLyFiQlpLEpIIsKBj8Z1rbO6mubwma4r79cN/q6gaOdnSdtH2SQWZqMplpwZSaTGZaCpmpSWSlpZCXkUJ+VhrjMlPfbk0VXWmemUZuRgpJo6TIKsxkkAKcB/y5u79iZvcAXwS+HLVNX3+Fk27u3H05sBwiXViHEKuIJListBTmlo9jbvm4E5Z3dzv7m9qoqmthb2Mrre1dHO3o4mh7ZGrt6KKtveuE5bVH2thW20lja/sJ/U31lmRQkJVGaV4G4/PSGT8ug9LcDMaPy2B8XgZleZH5gqzU0O9CwkwGe4A97v5K8P4RIsmg9zaTo95PAvYhIjJKJCUZ5fmZlA+hMjxaZ1c3TW2dQSuqSGV4Y9CS6vDRDuqb26ltamN/Uxsb9h6mvrl3aXqk08LSvHRuu2gKf3LxtDM9pT6Flgzcfb+Z7TazWe6+BbgCeKPXZo8DnzaznxCpOD6s+gIRGUtSkpMozE4b9NCo7Z3d1DUfY//hNg40tR1/PdDUNqjK9dOOM7Q9R/w58KOgJVEVcLuZLQVw92XAk0SalW4j0rT09pDjEREZ1dJSkobcLHc4hJoM3H0d0HuYtWVR6x34VJgxiIjIqamTEBERUTIQERElAxERQclARERQMhAREZQMREQEJQMREQEs0tQ/fphZHbDzND9eDNQPYzijwVg7p7F2PjD2zmmsnQ+MvXPq63wq3b2kvw/EXTI4E2a2xt17PwQX18baOY2184Gxd05j7Xxg7J3T6ZyPiolERETJQEREEi8ZLI91ACEYa+c01s4Hxt45jbXzgbF3TkM+n4SqMxARkb4l2p2BiIj0QclAREQSJxmY2TVmtsXMtplZ7+E345KZ7TCzDWa2zszWxDqeoTKz75lZrZltjFpWaGbPmNnW4HUIQ5rHXj/n9FUz2xtcp3Vmdl0sYxwKM5tsZr81s81mtsnM7gqWx+V1GuB84vkaZZjZajNbH5zT3wfLh3SNEqLOwMySgbeAq4iMu/wq8BF37z0MZ1wxsx3AQnePy4dlzOwSoBl4wN3nBcu+ATS4+9eDpF3g7l+IZZxD0c85fRVodvdvxjK202FmE4AJ7r7WzHKB14A/AG4jDq/TAOfzIeL3GhmQ7e7NZpYK/A64C3g/Q7hGiXJnsAjY5u5V7t4O/AS4KcYxJTx3fwFo6LX4JuAHwfwPiPxHjRv9nFPccvcad18bzB8BNgMTidPrNMD5xC2PaA7epgaTM8RrlCjJYCKwO+r9HuL8H0DAgRVm9pqZ3RnrYIZJmbvXQOQ/LlAa43iGy6fN7PWgGCkuilR6M7MpwDuAVxgD16nX+UAcXyMzSzazdUAt8Iy7D/kaJUoysD6WjYXysXe5+3nAtcCngiIKGX3uA6YD5wI1wL/GNJrTYGY5wKPAZ9y9KdbxnKk+zieur5G7d7n7ucAkYJGZzRvqPhIlGewBJke9nwTsi1Esw8bd9wWvtcDPiBSHxbsDQbluT/lubYzjOWPufiD4z9oNfIc4u05BOfSjwI/c/bFgcdxep77OJ96vUQ93bwRWAtcwxGuUKMngVWCmmU01szTgZuDxGMd0RswsO6gAw8yygauBjQN/Ki48Dnw8mP848IsYxjIsev5DBt5HHF2noHLyfmCzu/9b1Kq4vE79nU+cX6MSM8sP5jOBK4E3GeI1SojWRABBU7H/AJKB77n7P8U2ojNjZtOI3A0ApAA/jrdzMrOHgCVEuts9AHwF+DnwMFAB7AI+6O5xUyHbzzktIVL84MAO4BM9ZbmjnZm9G3gR2AB0B4u/RKScPe6u0wDn8xHi9xrNJ1JBnEzkB/7D7v41MytiCNcoYZKBiIj0L1GKiUREZABKBiIiomQgIiJKBiIigpKBiIigZCBynJl1RfVauW44e7c1synRPZmKjDYpsQ5AZBQ5GjzSL5JwdGcgcgrBuBF3B33GrzazGcHySjN7Lujc7DkzqwiWl5nZz4L+5deb2UXBrpLN7DtBn/MrgqdFMbO/MLM3gv38JEanKQlOyUDkbZm9iok+HLWuyd0XAd8i8iQ7wfwD7j4f+BFwb7D8XuB5d18AnAdsCpbPBP7L3ecCjcAfBsu/CLwj2M/ScE5NZGB6AlkkYGbN7p7Tx/IdwOXuXhV0crbf3YvMrJ7IQCkdwfIady82szpgkrsfi9rHFCJdC88M3n8BSHX3fzSzp4kMiPNz4OdRfdOLjBjdGYgMjvcz3982fTkWNd/F23V21wP/BZwPvGZmqsuTEadkIDI4H456fTmYf4lID7gAHyMy3CDAc8An4figI3n97dTMkoDJ7v5b4PNAPnDS3YlI2PQLRORtmcFoUT2edvee5qXpZvYKkR9QHwmW/QXwPTP7HFAH3B4svwtYbmZ/TOQO4JNEBkzpSzLwoJmNIzII078HfdKLjCjVGYicQlBnsNDd62Mdi0hYVEwkIiK6MxAREd0ZiIgISgYiIoKSgYiIoGQgIiIoGYiICPD/AUGnNijkgg6LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 278,
       "width": 387
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the changes of training losses over the 30 epochs. It can be seen that the losses first increase in the first 3 epochs, which then decreased rapidly until epoch 10. Afterwards, the losses decreased slowly became stable when it approaches 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00020-96f18537-ff34-4756-a01f-49268c38f088",
    "deepnote_cell_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was home and i was a little of the whole of the whole of the whole of the whole of the whole of the whole of the whole of the life shit at him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him\n"
     ]
    }
   ],
   "source": [
    "print(text_generator('i was home', 100, \n",
    "                     tokenizer, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the texts generated by the initial model have a lot of word repetitions, which calls for further tuning as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-b24a1a93-2c66-400e-83ac-891c37697594",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### (2) After some tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cell_id": "00022-62c11668-bf0e-4370-a24f-fd535617a04f",
    "deepnote_cell_type": "code",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1214/1214 [==============================] - 22s 16ms/step - loss: 6.7461 - accuracy: 0.0746\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.74614, saving model to model_weights117.hdf5\n",
      "Epoch 2/25\n",
      "1214/1214 [==============================] - 19s 16ms/step - loss: 5.8955 - accuracy: 0.1180\n",
      "\n",
      "Epoch 00002: loss improved from 6.74614 to 5.89552, saving model to model_weights117.hdf5\n",
      "Epoch 3/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 5.2114 - accuracy: 0.1451\n",
      "\n",
      "Epoch 00003: loss improved from 5.89552 to 5.21139, saving model to model_weights117.hdf5\n",
      "Epoch 4/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 4.3986 - accuracy: 0.1889\n",
      "\n",
      "Epoch 00004: loss improved from 5.21139 to 4.39860, saving model to model_weights117.hdf5\n",
      "Epoch 5/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 3.5887 - accuracy: 0.2801\n",
      "\n",
      "Epoch 00005: loss improved from 4.39860 to 3.58872, saving model to model_weights117.hdf5\n",
      "Epoch 6/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 2.9784 - accuracy: 0.3680\n",
      "\n",
      "Epoch 00006: loss improved from 3.58872 to 2.97840, saving model to model_weights117.hdf5\n",
      "Epoch 7/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 2.5490 - accuracy: 0.4387\n",
      "\n",
      "Epoch 00007: loss improved from 2.97840 to 2.54897, saving model to model_weights117.hdf5\n",
      "Epoch 8/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 2.2549 - accuracy: 0.4908\n",
      "\n",
      "Epoch 00008: loss improved from 2.54897 to 2.25493, saving model to model_weights117.hdf5\n",
      "Epoch 9/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 2.0277 - accuracy: 0.5328\n",
      "\n",
      "Epoch 00009: loss improved from 2.25493 to 2.02770, saving model to model_weights117.hdf5\n",
      "Epoch 10/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.8758 - accuracy: 0.5606\n",
      "\n",
      "Epoch 00010: loss improved from 2.02770 to 1.87583, saving model to model_weights117.hdf5\n",
      "Epoch 11/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.7571 - accuracy: 0.5819\n",
      "\n",
      "Epoch 00011: loss improved from 1.87583 to 1.75707, saving model to model_weights117.hdf5\n",
      "Epoch 12/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.6571 - accuracy: 0.6012\n",
      "\n",
      "Epoch 00012: loss improved from 1.75707 to 1.65715, saving model to model_weights117.hdf5\n",
      "Epoch 13/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.5794 - accuracy: 0.6129\n",
      "\n",
      "Epoch 00013: loss improved from 1.65715 to 1.57942, saving model to model_weights117.hdf5\n",
      "Epoch 14/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.5503 - accuracy: 0.6194\n",
      "\n",
      "Epoch 00014: loss improved from 1.57942 to 1.55033, saving model to model_weights117.hdf5\n",
      "Epoch 15/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.4836 - accuracy: 0.6328\n",
      "\n",
      "Epoch 00015: loss improved from 1.55033 to 1.48360, saving model to model_weights117.hdf5\n",
      "Epoch 16/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.4522 - accuracy: 0.6360\n",
      "\n",
      "Epoch 00016: loss improved from 1.48360 to 1.45222, saving model to model_weights117.hdf5\n",
      "Epoch 17/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.4236 - accuracy: 0.6419\n",
      "\n",
      "Epoch 00017: loss improved from 1.45222 to 1.42361, saving model to model_weights117.hdf5\n",
      "Epoch 18/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.4260 - accuracy: 0.6393\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.42361\n",
      "Epoch 19/25\n",
      "1214/1214 [==============================] - 20s 17ms/step - loss: 1.3878 - accuracy: 0.6459\n",
      "\n",
      "Epoch 00019: loss improved from 1.42361 to 1.38780, saving model to model_weights117.hdf5\n",
      "Epoch 20/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.3445 - accuracy: 0.6561\n",
      "\n",
      "Epoch 00020: loss improved from 1.38780 to 1.34446, saving model to model_weights117.hdf5\n",
      "Epoch 21/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.3478 - accuracy: 0.6556\n",
      "\n",
      "Epoch 00021: loss did not improve from 1.34446\n",
      "Epoch 22/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.3456 - accuracy: 0.6533\n",
      "\n",
      "Epoch 00022: loss did not improve from 1.34446\n",
      "Epoch 23/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.3421 - accuracy: 0.6558\n",
      "\n",
      "Epoch 00023: loss improved from 1.34446 to 1.34214, saving model to model_weights117.hdf5\n",
      "Epoch 24/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.3477 - accuracy: 0.65190s - l\n",
      "\n",
      "Epoch 00024: loss did not improve from 1.34214\n",
      "Epoch 25/25\n",
      "1214/1214 [==============================] - 20s 16ms/step - loss: 1.3310 - accuracy: 0.6538\n",
      "\n",
      "Epoch 00025: loss improved from 1.34214 to 1.33101, saving model to model_weights117.hdf5\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x0000028FA5F4E2E0>\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 34, 100)           1066400   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 400)               481600    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10664)             4276264   \n",
      "=================================================================\n",
      "Total params: 5,824,264\n",
      "Trainable params: 5,824,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construct the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, # Length of vector used in embedding\n",
    "                                      # is reduced from 150 to 100\n",
    "                    input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(200))) # Only one bidirectional LSTM is \n",
    "                                    # used without Dropout\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(learning_rate=0.005) # Learning rate is decreased\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Save the second model\n",
    "filepath = \"model_weights2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(xs, ys, epochs=25, batch_size=64, verbose=1, \n",
    "                    callbacks=desired_callbacks)\n",
    "\n",
    "print(model)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "cell_id": "00023-f4060d42-3173-45cc-bcfb-fcc17f8032a1",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 278,
       "width": 380
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that as the number of epoch increases, the training losses have been decreasing first rapidly then steadily. When it approaches 25 epochs, the losses are becoming stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00024-442873ba-317c-4920-b74c-0555afff7eab",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was home and bastards for the ever time in alone from hung when i was still tired and i would desk a nick libra look lazing on an ones comming and until use reach late out of the fat and college watched a shows is going to day utilize and her until the is off the send sign in a called and a better contrasted to emo answered and seeing to my house forward i was just ovulation about i had my married a did for me and i without to bad 45 stand and give the easy hear the slow i\n"
     ]
    }
   ],
   "source": [
    "print(text_generator('i was home', 100, tokenizer, model_1, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00025-24a63d65-47b3-49df-85a1-da8a9b39334f",
    "deepnote_cell_type": "code"
   },
   "source": [
    "Compared to the text generated before, the one output by the tuned model hasn't shown problems of repeating words. Moreover, some phrases can be observed in the block of text. However, it appears that the text doesn't make much sense as sentence structures are still missing."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Course 3 - Week 4 - Lesson 2 - Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6076a159-ab5a-4771-a836-5f8518d260c1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
