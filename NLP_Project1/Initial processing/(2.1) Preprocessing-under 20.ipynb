{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is used to preprocess the three datasets output by the notebook **Initial analysis of dataset.ipynb**, and while there are three notebooks for this preprocessing steps in total (for each age group's data file), the **relevant detailed comments are only shown in this one**, as the other two are identical in terms of the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aayushmarishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these are the team members:   drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o= optimist p= pessimist  my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234899</th>\n",
       "      <td>urllink    ah jun.. &amp;.. hung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234900</th>\n",
       "      <td>urllink    glen chong &amp; kris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234901</th>\n",
       "      <td>urllink    ah jun~!!.. hahaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234902</th>\n",
       "      <td>urllink    come~!.. let me i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234903</th>\n",
       "      <td>urllink    yi biao~!.. hahaz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234904 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0                  info has been found (+/- 100 pages,...\n",
       "1                  these are the team members:   drewe...\n",
       "2                  in het kader van kernfusie op aarde...\n",
       "3                        testing!!!  testing!!!          \n",
       "4                         o= optimist p= pessimist  my...\n",
       "...                                                   ...\n",
       "234899                    urllink    ah jun.. &.. hung...\n",
       "234900                    urllink    glen chong & kris...\n",
       "234901                    urllink    ah jun~!!.. hahaz...\n",
       "234902                    urllink    come~!.. let me i...\n",
       "234903                    urllink    yi biao~!.. hahaz...\n",
       "\n",
       "[234904 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/blog_under_20.csv')\n",
    "\n",
    "# Convert all texts to lower case for convenience\n",
    "data['text'] = data.apply(lambda row: row.text.lower(), axis=1)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initial overview of the data files, we noticed several points that need to be preprocessed/filtered out before we carry on:\n",
    "1. Some of the blog posts are not written in English.\n",
    "    As most of the blogs are in English, it is expected that these non-English posts could have an inverse effect on the model training. Therefore, posts written in other languages should be filtered out.\n",
    "2. Many of the texts in the dataset are not \"clean\" and they often contain the keyword \"urllink\".\n",
    "    Apparently, paragraphs that contain this keyword should also be filtered out to avoid negative impact on the modelling. Considering that the dataset is fairly large (in the sub-dataset of posts under 20, there are 234,000+ observations), filtering these paragraphs will not affect the size of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filtering out non-English paragraphs\n",
    "Since most of the blogs are written in English, a simple test is used here to determine whether a paragraph is in English: a paragraph is considered to be English if it contains the words \"the\" and the word \"and\", which are two fairly unique words for English compared to other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_english = data.loc[data.text.str.contains(\"the\")\\\n",
    "                         ].loc[data.text.str.contains(\"and\")\\\n",
    "                              ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the texts into sentences and further filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences\n",
    "sentence_list = []\n",
    "blogs = filter_english['text'].tolist()\n",
    "for b in blogs:\n",
    "    # nltk.tokenize.sent_tokenize can split paragraphs into sentences\n",
    "    # according to common sentence-ending punctuations\n",
    "    sentences = nltk.tokenize.sent_tokenize(b)\n",
    "    \n",
    "    # Filter out sentences that include the word 'urllink'\n",
    "    sentences = ['' if 'urllink' in s else s for s in sentences]\n",
    "    \n",
    "    # Remove punctuations and filter length (both a lower and upper\n",
    "    # limit are imposed to make the data more consistent)\n",
    "    sentences = [s.strip(string.punctuation\\\n",
    "                        ).strip() if (len(s)>50 and len(s) < 150\\\n",
    "                                     ) else '' for s in sentences]\n",
    "    \n",
    "    # Filter out empty sentences\n",
    "    sentences = list(filter(None, sentences))\n",
    "    sentence_list += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1183933 sentences in total\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d sentences in total\" % (len(sentence_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the above for loop\n",
    "In this section, one of the paragraphs is used to illustrate the preprocessing of the above block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                  i've fallen so deep, so fast. i don't know what to do with myself. but i know i feel so good. i love to look at him, listen to the voice of the aries. i wish i had the courage to go up to him and kiss him. i want to, but yet fade away into the shadows of fear and questioning. dose aries like me? how can i tell? what if he doesn't and i'm making a fool of myself, stumbling over my feelings. a little girl with a crush just out of her reach. what if he dose like me too? if he asks me out do i say yes? of course i do! even though we will not see eachother as much as we'd like, who says it cant work! right...? but what if it won't work... i guess we'll have to find out...                \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example paragraph\n",
    "display(blogs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"                  i've fallen so deep, so fast.\",\n",
       " \"i don't know what to do with myself.\",\n",
       " 'but i know i feel so good.',\n",
       " 'i love to look at him, listen to the voice of the aries.',\n",
       " 'i wish i had the courage to go up to him and kiss him.',\n",
       " 'i want to, but yet fade away into the shadows of fear and questioning.',\n",
       " 'dose aries like me?',\n",
       " 'how can i tell?',\n",
       " \"what if he doesn't and i'm making a fool of myself, stumbling over my feelings.\",\n",
       " 'a little girl with a crush just out of her reach.',\n",
       " 'what if he dose like me too?',\n",
       " 'if he asks me out do i say yes?',\n",
       " 'of course i do!',\n",
       " \"even though we will not see eachother as much as we'd like, who says it cant work!\",\n",
       " 'right...?',\n",
       " \"but what if it won't work... i guess we'll have to find out...\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the nltk function to split the example paragraph into sentences\n",
    "sentences = nltk.tokenize.sent_tokenize(blogs[5])\n",
    "display(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " 'i love to look at him, listen to the voice of the aries',\n",
       " 'i wish i had the courage to go up to him and kiss him',\n",
       " 'i want to, but yet fade away into the shadows of fear and questioning',\n",
       " '',\n",
       " '',\n",
       " \"what if he doesn't and i'm making a fool of myself, stumbling over my feelings\",\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " \"even though we will not see eachother as much as we'd like, who says it cant work\",\n",
       " '',\n",
       " \"but what if it won't work... i guess we'll have to find out\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Strip the sentences of their punctuations and add a length filter\n",
    "sentences = [s.strip(string.punctuation\\\n",
    "                    ).strip() if (len(s)>50 and len(s) < 150\\\n",
    "                                 ) else '' for s in sentences]\n",
    "display(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love to look at him, listen to the voice of the aries',\n",
       " 'i wish i had the courage to go up to him and kiss him',\n",
       " 'i want to, but yet fade away into the shadows of fear and questioning',\n",
       " \"what if he doesn't and i'm making a fool of myself, stumbling over my feelings\",\n",
       " \"even though we will not see eachother as much as we'd like, who says it cant work\",\n",
       " \"but what if it won't work... i guess we'll have to find out\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete sentences that have been turned into empty strings\n",
    "sentences = list(filter(None, sentences))\n",
    "display(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output of the last cell shows, the preprocessing step splits the paragraph into sentences, remove punctuations, and in the end only keep sentences with certain lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1183933 sentences (observations) after the preprocessing\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(sentence_list)\n",
    "print('There are %d sentences (observations) after the preprocessing'\\\n",
    "      %df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n = 10000).reset_index(drop=True)\n",
    "sample.to_csv('../Data/sample_under_20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = df.sample(n = 5000).reset_index(drop=True)\n",
    "sample2.to_csv('../Data/sample2_under_20.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Course 3 - Week 4 - Lesson 2 - Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
